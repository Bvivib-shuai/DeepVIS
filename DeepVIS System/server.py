import os
import json
import sqlite3
import logging
import sys
from tqdm import tqdm
import pandas as pd

from flask import Flask, request, jsonify
from flask_cors import CORS

from unsloth import FastLanguageModel, is_bfloat16_supported
from transformers import TextStreamer
from peft import PeftModel

# -------------------------------------------
# Basic Configuration and Path Settings
# -------------------------------------------
app = Flask(__name__)
CORS(app)

# Current Project Root Directory
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Node and Chart Files Required for Static File Mode
NODES_FILE = os.path.join(BASE_DIR, "case1_node1.json")
SVG_FILE = os.path.join(BASE_DIR, "sample_chart.svg")

# File Paths and Model Configuration for Large Model Generation
DB_FILE = os.path.join(BASE_DIR, "uploaded_database.sqlite")
QUESTION_FILE = os.path.join(BASE_DIR, "question.json")                
CORRECTIONS_FILE = os.path.join(BASE_DIR, "corrections.json")           

# Intermediate Files in the Generation Phase (Generated Prompt, Response, Node)
GENERATED_PROMPT_FILE = os.path.join(BASE_DIR, "generated_prompt.json")
MODEL_RESPONSE_FILE = os.path.join(BASE_DIR, "model_response.json")
NODE_JSON_FILE = os.path.join(BASE_DIR, "node_3.json")
OUTPUT_TXT_FILE = os.path.join(BASE_DIR, "output.txt")

GENERATED_PROMPT_NEW_FILE = os.path.join(BASE_DIR, "generated_prompt_new.json")
MODEL_RESPONSE_NEW_FILE = os.path.join(BASE_DIR, "model_response_new.json")
NODE_JSON_NEW_FILE = os.path.join(BASE_DIR, "node_3_new.json")

# Model Configuration
SAVED_MODEL_FOLDER = os.path.join("value_rule_cot_model_2025-03-21_23-17-52")
SAVED_ADAPTER_FOLDER = os.path.join("output/checkpoint-788")

# -------------------------------------------
# Dynamic Generation vs. Static Display
# When DYNAMIC_MODE is True, the backend follows the process generated by the large model; when False, it directly returns static data.
# -------------------------------------------
DYNAMIC_MODE = True

import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import re
from io import StringIO
import calendar
from datetime import datetime

def vql_to_svg(db_file, node_json_file):
    with open(node_json_file, 'r', encoding='utf-8') as file:
        data = json.load(file)
    for item in data:
        if item.get('Name') == 'Generate Visualization':
            vql_query = item.get('Result')
            vql_query = vql_query.replace('[red]', '').replace('[/red]', '')
    
    def parse_vql(vql_query):
        vql_query = ' '.join(vql_query.split())
        
        column_pattern = r'[a-zA-Z_][a-zA-Z0-9_]*'
        agg_function_pattern = r'(?:COUNT|SUM|AVG|MIN|MAX|GROUP_CONCAT)\s*\(\s*(?:\*|' + column_pattern + r')\s*\)'
        column_or_agg_pattern = f'(?:{agg_function_pattern}|{column_pattern})'
        
        parts = {
            'visualize': r'^Visualize\s+(?P<chart_type>line|bar|pie|scatter)\s+',
            'select': rf'SELECT\s+(?P<select_cols>{column_or_agg_pattern}\s*,\s*{column_or_agg_pattern})\s+',
            'from': rf'FROM\s+(?P<from_tables>{column_pattern})\s*',
            'where': r'(?:WHERE\s+(?P<where_cond>(?:(?!GROUP\s+BY|ORDER\s+BY|LIMIT|BIN).)+)\s*)?',
            'group': rf'(?:GROUP\s+BY\s+(?P<group_cols>{column_or_agg_pattern})\s*)?',
            'order': rf'(?:ORDER\s+BY\s+(?P<order_cols>{column_or_agg_pattern}))(?:\s+(?P<order_dir>ASC|DESC)\s*)?',
            'limit': r'(?:LIMIT\s+(?P<limit_num>\d+)\s*)?',
            'bin': rf'(?:BIN\s+(?P<bin_col>{column_pattern})\s+BY\s+(?P<bin_unit>day|weekday|month|year)\s*)?$'
        }
        
        result = {}
        remaining_query = vql_query
        
        try:
            match = re.match(parts['visualize'], remaining_query, re.IGNORECASE)
            if not match:
                raise ValueError("Invalid VISUALIZE clause. Must be: Visualize line|bar|pie|scatter")
            result.update(match.groupdict())
            remaining_query = remaining_query[match.end():].strip()
            
            match = re.match(parts['select'], remaining_query, re.IGNORECASE)
            if not match:
                raise ValueError("Invalid SELECT clause. Must specify exactly two columns/aggregations separated by comma")
            result.update(match.groupdict())
            remaining_query = remaining_query[match.end():].strip()
            
            select_columns = [col.strip() for col in result['select_cols'].split(',')]
            if len(select_columns) != 2:
                raise ValueError("SELECT must specify exactly two columns/aggregations separated by comma")
            
            match = re.match(parts['from'], remaining_query, re.IGNORECASE)
            if not match:
                raise ValueError(f"Invalid FROM clause. Must specify one table name. Remaining: '{remaining_query}'")
            result.update(match.groupdict())
            remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query.startswith('WHERE'):
                match = re.match(parts['where'], remaining_query, re.IGNORECASE)
                if not match:
                    raise ValueError("Invalid WHERE clause")
                result.update({k:v for k,v in match.groupdict().items() if v})
                remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query.startswith('GROUP BY'):
                match = re.match(parts['group'], remaining_query, re.IGNORECASE)
                if not match:
                    raise ValueError("Invalid GROUP BY clause")
                result.update({k:v for k,v in match.groupdict().items() if v})
                remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query.startswith('ORDER BY'):
                match = re.match(parts['order'], remaining_query, re.IGNORECASE)
                if not match:
                    raise ValueError("Invalid ORDER BY clause")
                result.update({k:v for k,v in match.groupdict().items() if v})
                remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query.startswith('LIMIT'):
                match = re.match(parts['limit'], remaining_query, re.IGNORECASE)
                if not match:
                    raise ValueError("Invalid LIMIT clause")
                result.update({k:v for k,v in match.groupdict().items() if v})
                remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query.startswith('BIN'):
                match = re.match(parts['bin'], remaining_query, re.IGNORECASE)
                if not match:
                    raise ValueError("Invalid BIN clause")
                result.update({k:v for k,v in match.groupdict().items() if v})
                remaining_query = remaining_query[match.end():].strip()
            
            if remaining_query:
                raise ValueError(f"Unrecognized query parts: {remaining_query}")
            
            return result
        
        except Exception as e:
            raise ValueError(f"Query parsing failed: {str(e)}")

    def execute_query(db_path, parsed_vql):
        try:
            select_cols = parsed_vql['select_cols']
            from_tables = parsed_vql['from_tables']
            
            if parsed_vql.get('bin_col'):
                bin_col = parsed_vql['bin_col']
                bin_unit = parsed_vql['bin_unit'].lower()
                
                if bin_unit == 'day':
                    bin_expr = f"date({bin_col})" 
                elif bin_unit == 'weekday':
                    bin_expr = f"strftime('%w', {bin_col})"
                elif bin_unit == 'month':
                    bin_expr = f"strftime('%m', {bin_col})"  
                elif bin_unit == 'year':
                    bin_expr = f"strftime('%Y', {bin_col})"  
                
                select_cols = select_cols.replace(bin_col, bin_expr)
                
                if not parsed_vql.get('group_cols'):
                    parsed_vql['group_cols'] = bin_expr
            
            sql = f"SELECT {select_cols} FROM {from_tables}"
            
            if parsed_vql.get('where_cond'):
                sql += f" WHERE {parsed_vql['where_cond']}"
            
            if parsed_vql.get('group_cols'):
                sql += f" GROUP BY {parsed_vql['group_cols']}"
            
            if parsed_vql.get('order_cols'):
                order_dir = f" {parsed_vql.get('order_dir', '')}"
                sql += f" ORDER BY {parsed_vql['order_cols']}{order_dir}"
            
            if parsed_vql.get('limit_num'):
                sql += f" LIMIT {parsed_vql['limit_num']}"
            
            conn = sqlite3.connect(db_path)
            
            df = pd.read_sql_query(sql, conn)
            
            conn.close()
            
            return df
        
        except sqlite3.Error as e:
            raise ValueError(f"Database error: {str(e)}")
        except Exception as e:
            raise ValueError(f"Query execution failed: {str(e)}")

    def generate_svg(df, parsed_vql):
        try:
            plt.figure(figsize=(10, 6))
            chart_type = parsed_vql['chart_type'].lower()
            x_col = df.columns[0]
            y_col = df.columns[1]
            
            if parsed_vql.get('bin_col'):
                bin_unit = parsed_vql['bin_unit']
                
                if bin_unit == 'weekday':
                    weekday_map = {
                        '0': 'Sun', '1': 'Mon', '2': 'Tue',
                        '3': 'Wed', '4': 'Thu', '5': 'Fri', '6': 'Sat'
                    }
                    df[x_col] = df[x_col].astype(str).map(weekday_map)
                elif bin_unit == 'month':
                    df[x_col] = df[x_col].apply(lambda x: calendar.month_abbr[int(x)])
            
            if chart_type == 'line':
                plt.plot(df[x_col], df[y_col], marker='o')
                plt.grid(True)
            elif chart_type == 'bar':
                plt.bar(df[x_col], df[y_col])
            elif chart_type == 'pie':
                plt.pie(df[y_col], labels=df[x_col], autopct='%1.1f%%')
            elif chart_type == 'scatter':
                plt.scatter(df[x_col], df[y_col])
            
            plt.xticks(rotation=45 if len(df[x_col]) > 5 else 0)
            plt.tight_layout()
            
            svg_buffer = StringIO()
            plt.savefig(svg_buffer, format='svg', bbox_inches='tight')
            plt.close()
            
            return svg_buffer.getvalue()
        
        except Exception as e:
            plt.close()
            raise ValueError(f"Chart generation failed: {str(e)}")

    try:
        parsed_vql = parse_vql(vql_query)
        df = execute_query(db_file, parsed_vql)
        svg_data = generate_svg(df, parsed_vql)
        return svg_data
    except Exception as e:
        raise ValueError(f"VQL to SVG conversion failed: {str(e)}")

def generate_database_schema(db_file):
    logging.debug(f"[Generate] Database Description，Database File：{db_file}")
    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        schema = {}
        for table in tables:
            table_name = table[0]
            logging.debug(f"[Generate] Processing Tables：{table_name}")
            cursor.execute(f"PRAGMA table_info({table_name});")
            columns = cursor.fetchall()
            column_info = []
            for column in columns:
                column_name = column[1]
                column_type = column[2]
                column_info.append(f"{column_name}: {column_type}")
            schema[table_name] = column_info
        conn.close()
        logging.debug("[Generate] Generation of Database Description Completed")
        return schema
    except sqlite3.Error as e:
        logging.error(f"[Generate] SQLite Error: {e}")
        return None
    except Exception as e:
        logging.error(f"[Generate] Unknown Error: {e}")
        return None

def format_schema(schema):
    logging.debug("[Generate] Starting to format the database.")
    lines = []
    for table, columns in schema.items():
        columns_str = '; '.join(columns)
        lines.append(f"Table {table}: [{columns_str}]")
    logging.debug("[Generate] Database schema formatting completed")
    return lines

def build_prompt(question, db_schema_lines):
    logging.debug("[Generate] build prompt for generation")
    constraint_text = """Please generate VQL by reasoning step by step, based on the following Question, Database Schema, and Constraints.
Question:
[question]

Database Schema:
[db_schema]

The VQL template is as follows:  
Visualize [VISUALIZE_TYPE] 
SELECT [SELECT_COLUMNS] 
FROM [FROM_TABLES] 
WHERE [WHERE_CONDITION] 
GROUP BY [GROUP_BY_COLUMNS] 
ORDER BY [ORDER_BY_COLUMNS] [ORDER_BY_DIRECTION] 
LIMIT [LIMIT_NUMBER] 
BIN [BIN_COLUMN] BY [BIN_TIME_UNIT] 

The constraints for each part are as follows:
1. VISUALIZE field: Only BAR, PIE, LINE, and SCATTER are allowed as values. It is used to specify the data visualization chart type of the query results. Analyze the nature of the question and the database schema to explain why the [TYPE] in the Pre - entered Correct VQL is chosen.
2. SELECT field: There must be exactly two columns from the database in the SELECT clause. These columns must be either directly from the columns listed in the Database Schema or valid derivations based on those columns (COUNT, AVG, MAX and MIN). Explain why the [COLUMNS] in the Pre - entered Correct VQL are selected considering the question and the database schema.
3. FROM field: Only add the table names that must be used to complete the natural - language query. Evaluate the question and the database schema to explain why the [TABLES] in the Pre - entered Correct VQL are necessary to answer the question. Describe the connection between these tables and the data required by the question.
4. WHERE field: Analyze the natural - language query and the database schema to determine whether there is a filtering requirement. When there are clear limiting conditions, accurately explain why the WHERE clause logic expressions in the Pre - entered Correct VQL are formed this way, considering the conditional logical relationships to match the query intent. The columns used in the conditions must be from the Database Schema or valid derivations.
5. Grouping:
    If you need to group or aggregate the time column in the SELECT clause by a time unit (day, weekday, month, or year) and the current table does not have a column that meets this time - unit requirement, then use the BIN [BIN_COLUMN] BY [BIN_TIME_UNIT] part. [BIN_COLUMN] should be a time column from the Database Schema, and [BIN_TIME_UNIT] should be the appropriate time unit (day, weekday, month, or year).
    When the dataset needs to be grouped and it is not related to time, add other columns in the GROUP BY clause as grouping bases in sequence, with earlier - used grouping bases placed more forward. The columns in the GROUP BY clause must be from the Database Schema or valid derivations. Explain why the GROUP BY and BIN (if applicable) parts in the Pre - entered Correct VQL are set up based on the question and the database schema.
6. LIMIT field: Add a LIMIT clause at the end of the VQL to specify the maximum number of rows to return. If there is no limit requirement, leave it empty. Explain why the LIMIT clause (if present) in the Pre - entered Correct VQL is included or not, considering factors such as the amount of data needed to answer the question and performance implications.

Now, please reason according to the following strict format:  
Step 1: 
Reasoning for Chart Type: [Explain why the chart type in the Pre - entered Correct VQL is chosen based on the question and the database schema]
Chart Type: [Fill in the chart type from the Pre - entered Correct VQL here]

Step 2: 
Reasoning for FROM: [Explain why the tables in the FROM clause of the Pre - entered Correct VQL are chosen based on the question and the database schema]
FROM: [Table names for the FROM clause from the Pre - entered Correct VQL]
Reasoning for SELECT: [Explain why the columns in the SELECT clause of the Pre - entered Correct VQL are chosen based on the question and the database schema]
SELECT: [Columns for the SELECT clause from the Pre - entered Correct VQL]
Reasoning for WHERE: [Explain why the conditions in the WHERE clause of the Pre - entered Correct VQL are set based on the question and the database schema]
WHERE: [Conditions for the WHERE clause from the Pre - entered Correct VQL]

Step 3: 
Reasoning for GROUP BY: [Explain why the columns in the GROUP BY clause of the Pre - entered Correct VQL are used for grouping based on the question and the database schema]
GROUP BY: [Columns for the GROUP BY clause from the Pre - entered Correct VQL]
Reasoning for BIN: [If applicable, explain why the BIN_COLUMN and BIN_TIME_UNIT in the Pre - entered Correct VQL are used based on the time - related grouping requirements and the database schema]
BIN: [BIN_COLUMN from the Pre - entered Correct VQL]
BY: [BIN_TIME_UNIT from the Pre - entered Correct VQL]

Step 4: 
Reasoning for ORDER BY: [Explain why the columns in the ORDER BY clause of the Pre - entered Correct VQL are used for sorting based on the question and the database schema]
ORDER BY: [Columns for the ORDER BY clause from the Pre - entered Correct VQL]
Reasoning for SORT DIRECTION: [Based on the analysis of the question requirements and the database schema, explain why the ASC or DESC (or empty) in the Pre - entered Correct VQL is used for each column in the ORDER BY clause]
SORT DIRECTION: [ASC|DESC|empty from the Pre - entered Correct VQL]
Reasoning for LIMIT: [Explain why the LIMIT clause (if present) in the Pre - entered Correct VQL is set as such based on various factors]
LIMIT: [Number of rows for the LIMIT clause from the Pre - entered Correct VQL]


Step 5:
Inspection: Check whether the reasoning results of the first four steps are reasonable. 
Final VQL：Output the final vql.
"""
    prompt = constraint_text.replace("[question]", question)
    prompt = prompt.replace("[db_schema]", "\n".join(db_schema_lines))
    logging.debug("[Generate] prompt completed")
    return prompt

def build_prompt_regenerate(question, db_schema_lines, user_feedback, original_response):
    logging.debug("[Regenerate] build prompt for regeneration")
    constraint_text = """Please regenerate VQL by reasoning step by step, based on the following Question, Database Schema, Constraints, User Feedback, and the Original Generated Response.
Question:
[question]

Database Schema:
[db_schema]

User Feedback:
[User Feedback]

Original Generated Response:
[Original Generated Response]

The VQL template is as follows:  
Visualize [VISUALIZE_TYPE] 
SELECT [SELECT_COLUMNS] 
FROM [FROM_TABLES] 
WHERE [WHERE_CONDITION] 
GROUP BY [GROUP_BY_COLUMNS] 
ORDER BY [ORDER_BY_COLUMNS] [ORDER_BY_DIRECTION] 
LIMIT [LIMIT_NUMBER] 
BIN [BIN_COLUMN] BY [BIN_TIME_UNIT] 

The constraints for each part are as follows:
1. VISUALIZE field: Only BAR, PIE, LINE, and SCATTER are allowed as values. It is used to specify the data visualization chart type of the query results. Analyze the nature of the question and the database schema to explain why the [TYPE] in the Pre - entered Correct VQL is chosen.
2. SELECT field: There must be exactly two columns from the database in the SELECT clause. These columns must be either directly from the columns listed in the Database Schema or valid derivations based on those columns (COUNT, AVG, MAX and MIN). Explain why the [COLUMNS] in the Pre - entered Correct VQL are selected considering the question and the database schema.
3. FROM field: Only add the table names that must be used to complete the natural - language query. Evaluate the question and the database schema to explain why the [TABLES] in the Pre - entered Correct VQL are necessary to answer the question. Describe the connection between these tables and the data required by the question.
4. WHERE field: Analyze the natural - language query and the database schema to determine whether there is a filtering requirement. When there are clear limiting conditions, accurately explain why the WHERE clause logic expressions in the Pre - entered Correct VQL are formed this way, considering the conditional logical relationships to match the query intent. The columns used in the conditions must be from the Database Schema or valid derivations.
5. Grouping:
    If you need to group or aggregate the time column in the SELECT clause by a time unit (day, weekday, month, or year) and the current table does not have a column that meets this time - unit requirement, then use the BIN [BIN_COLUMN] BY [BIN_TIME_UNIT] part. [BIN_COLUMN] should be a time column from the Database Schema, and [BIN_TIME_UNIT] should be the appropriate time unit (day, weekday, month, or year).
    When the dataset needs to be grouped and it is not related to time, add other columns in the GROUP BY clause as grouping bases in sequence, with earlier - used grouping bases placed more forward. The columns in the GROUP BY clause must be from the Database Schema or valid derivations. Explain why the GROUP BY and BIN (if applicable) parts in the Pre - entered Correct VQL are set up based on the question and the database schema.
6. LIMIT field: Add a LIMIT clause at the end of the VQL to specify the maximum number of rows to return. If there is no limit requirement, leave it empty. Explain why the LIMIT clause (if present) in the Pre - entered Correct VQL is included or not, considering factors such as the amount of data needed to answer the question and performance implications.

Now, please reason according to the following strict format:  
Step 1: 
Reasoning for Chart Type: [Explain why the chart type in the Pre - entered Correct VQL is chosen based on the question and the database schema]
Chart Type: [Fill in the chart type from the Pre - entered Correct VQL here]

Step 2: 
Reasoning for FROM: [Explain why the tables in the FROM clause of the Pre - entered Correct VQL are chosen based on the question and the database schema]
FROM: [Table names for the FROM clause from the Pre - entered Correct VQL]
Reasoning for SELECT: [Explain why the columns in the SELECT clause of the Pre - entered Correct VQL are chosen based on the question and the database schema]
SELECT: [Columns for the SELECT clause from the Pre - entered Correct VQL]
Reasoning for WHERE: [Explain why the conditions in the WHERE clause of the Pre - entered Correct VQL are set based on the question and the database schema]
WHERE: [Conditions for the WHERE clause from the Pre - entered Correct VQL]

Step 3: 
Reasoning for GROUP BY: [Explain why the columns in the GROUP BY clause of the Pre - entered Correct VQL are used for grouping based on the question and the database schema]
GROUP BY: [Columns for the GROUP BY clause from the Pre - entered Correct VQL]
Reasoning for BIN: [If applicable, explain why the BIN_COLUMN and BIN_TIME_UNIT in the Pre - entered Correct VQL are used based on the time - related grouping requirements and the database schema]
BIN: [BIN_COLUMN from the Pre - entered Correct VQL]
BY: [BIN_TIME_UNIT from the Pre - entered Correct VQL]

Step 4: 
Reasoning for ORDER BY: [Explain why the columns in the ORDER BY clause of the Pre - entered Correct VQL are used for sorting based on the question and the database schema]
ORDER BY: [Columns for the ORDER BY clause from the Pre - entered Correct VQL]
Reasoning for SORT DIRECTION: [Based on the analysis of the question requirements and the database schema, explain why the ASC or DESC (or empty) in the Pre - entered Correct VQL is used for each column in the ORDER BY clause]
SORT DIRECTION: [ASC|DESC|empty from the Pre - entered Correct VQL]
Reasoning for LIMIT: [Explain why the LIMIT clause (if present) in the Pre - entered Correct VQL is set as such based on various factors]
LIMIT: [Number of rows for the LIMIT clause from the Pre - entered Correct VQL]


Step 5:
Inspection: Check whether the reasoning results of the first four steps are reasonable. 
Final VQL：Output the final vql.
"""
    prompt = constraint_text.replace("[question]", question)
    prompt = prompt.replace("[db_schema]", "\n".join(db_schema_lines))
    prompt = prompt.replace("[User Feedback]", user_feedback)
    prompt = prompt.replace("[Original Generated Response]", original_response)
    logging.debug("[Regenerate] Prompt Generation Completed")
    return prompt

def deep_dict_to_json(obj):
    if isinstance(obj, dict):
        return {key: deep_dict_to_json(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [deep_dict_to_json(item) for item in obj]
    else:
        return obj

def load_json_data(filename: str):
    logging.debug(f"Load JSON File：{filename}")
    data = []
    with open(filename, 'r', encoding='utf-8') as file:
        test_data = json.load(file)
        for json_obj in test_data:
            json_obj = deep_dict_to_json(json_obj)
            json_obj = json.dumps(json_obj, ensure_ascii=False, indent=4)
            data.append(json.loads(json_obj))
    logging.debug("Complete JSON Load")
    return data

def extract_content_1(data_item):
    return data_item.get('content_1', {})

def generate_input(content_1):
    logging.debug("Generate Input for Model")
    return {"role": "user", "content": content_1}

def load_model(with_lora: bool = True, saved_model_folder=None, saved_adapter_folder=None):
    logging.debug("Begin To Load Model")
    max_seq_length = 4048
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=saved_model_folder,
        max_seq_length=max_seq_length,
        load_in_4bit=True,
        dtype=None,
    )
    logging.debug(f"Pad token: {tokenizer.pad_token}")
    logging.debug(f"EOS token: {tokenizer.eos_token}")
    model = FastLanguageModel.for_inference(model)
    if with_lora:
        logging.debug("Load LoRA Weight")
        model = PeftModel.from_pretrained(model, saved_adapter_folder)
    logging.debug("Load Model Successfully ")
    return model, tokenizer

def generate_responses(model, tokenizer, prompt):
    logging.debug("Begin To Generate Response")
    inputs = tokenizer.apply_chat_template(
        [prompt],
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )
    device = next(model.parameters()).device
    inputs = inputs.to(device)
    response = model.generate(input_ids=inputs, max_new_tokens=4048, use_cache=True, temperature=0.1)
    response_txt = tokenizer.decode(response[0], skip_special_tokens=True)
    logging.debug("Response Generation Completed")
    return response_txt

def process_json_file(file_path):
    logging.debug(f"Begin To Process Response File：{file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        response_text = data[0]['response_finetuned_model']
        assistant_index = response_text.find('assistant')
        if assistant_index != -1:
            extracted_content = response_text[assistant_index + len('assistant'):].strip()
            processed_content = extracted_content.replace('\\n', '\n')
            processed_content = processed_content.replace('"', "")
            with open(OUTPUT_TXT_FILE, 'w', encoding='utf-8') as output_file:
                output_file.write(processed_content)
            logging.debug(f"Saved Response Content to {OUTPUT_TXT_FILE} 文件中。")
            return OUTPUT_TXT_FILE
        else:
            logging.error("Not Find 'assistant' String。")
            return None
    except FileNotFoundError:
        logging.error(f"Not Find File: {file_path}")
        return None
    except json.JSONDecodeError:
        logging.error("JSON Erro")
        return None

def convert_txt_to_json(txt_data):
    logging.debug("Transform TXT Into JSON")
    json_data = {}
    lines = txt_data.splitlines()
    current_step = None
    current_step_data = {}
    for line in lines:
        line = line.strip()
        if line.startswith("Step "):
            if current_step:
                json_data[current_step] = current_step_data
            current_step = line.split(":")[0].strip()
            current_step_data = {}
        elif line:
            parts = line.split(":", 1)
            if len(parts) == 2:
                key = parts[0].strip()
                value = parts[1].strip()
                value = value.replace("(empty)", "[empty]")
                current_step_data[key] = value
            else:
                logging.debug(f"Skip Line: {line}")
    if current_step:
        json_data[current_step] = current_step_data
    logging.debug("Completed Format Transformation")
    return json_data

def convert_json_to_nodes(json_data):
    logging.debug("Start converting JSON data into a node structure")
    step_nodes = []
    field_nodes = []
    step_sub_node_mapping = {}
    step_name_mapping = {
        "Step 1": "Determine Chart",
        "Step 2": "Retrieve Data",
        "Step 3": "Define Granularity",
        "Step 4": "Refine Data"
    }

    for step_key in json_data:
        if step_key == "Step 5":
            continue
        step_sub_node_mapping[step_key] = []
        step_content = json_data[step_key]
        result_parts = []
        reasoning_parts = []
        all_result_parts = []
        for field_key, field_value in step_content.items():
            if "Reasoning for" in field_key:
                reasoning_parts.append(field_value)
            else:
                all_result_parts.append(f"{field_key} {field_value}")
                if field_value not in ['[empty]', None]:
                    result_parts.append(f"{field_key} {field_value}")
        result = ';'.join(result_parts)
        reasoning = ' '.join(reasoning_parts)
        step_node = {
            "Name": step_name_mapping.get(step_key, step_key),
            "Result": result,
            "Reasoning": reasoning,
            "Sub - node Names": "",
            "Status": 1,
            "Index": None,
            "Sub - node Indices": []
        }
        if step_node["Name"] == "Determine Chart":
            chart_type = [part for part in all_result_parts if "Chart Type" in part]
            if chart_type:
                chart_type_val = chart_type[0].split(" ")[-1]
                step_node["Result"] = f"VISUALIZE {chart_type_val}"
        step_node["_step_key"] = step_key
        step_nodes.append(step_node)

    for step_key in json_data:
        if step_key == "Step 5":
            continue
        step_content = json_data[step_key]
        bin_value = step_content.get("BIN", "")
        by_value = step_content.get("BY", "")
        bin_reasoning = step_content.get("Reasoning for BIN", "")
        by_reasoning = step_content.get("Reasoning for BY", "")
        if bin_value or by_value:
            combined_result = f"BIN {bin_value} BY {by_value}"
            combined_reasoning = f"{bin_reasoning} {by_reasoning}".strip()
            field_node = {
                "Name": f"BIN {bin_value} BY {by_value}",
                "Result": combined_result,
                "Reasoning": combined_reasoning,
                "Sub - node Names": "None",
                "Status": 1,
                "Index": None,
                "Sub - node Indices": [],
                "parent": step_key
            }
            field_nodes.append(field_node)
            step_sub_node_mapping[step_key].append(field_node["Name"])
        for field_key, field_value in step_content.items():
            if "Reasoning for" in field_key or field_key in ["BIN", "BY"]:
                continue
            if field_key == "Chart Type":
                reasoning = step_content.get("Reasoning for Chart Type", "")
                result_val = field_value
                field_node = {
                    "Name": f"VISUALIZE {result_val}",
                    "Result": result_val,
                    "Reasoning": reasoning,
                    "Sub - node Names": "None",
                    "Status": 1,
                    "Index": None,
                    "Sub - node Indices": [],
                    "parent": step_key
                }
                field_nodes.append(field_node)
                step_sub_node_mapping[step_key].append(field_node["Name"])
            else:
                reasoning_key = f"Reasoning for {field_key}"
                reasoning = step_content.get(reasoning_key, "")
                result_val = field_value
                field_node = {
                    "Name": f"{field_key} {result_val}",
                    "Result": result_val,
                    "Reasoning": reasoning,
                    "Sub - node Names": "None",
                    "Status": 1,
                    "Index": None,
                    "Sub - node Indices": [],
                    "parent": step_key
                }
                field_nodes.append(field_node)
                step_sub_node_mapping[step_key].append(field_node["Name"])

    for node in step_nodes:
        step_key = node["_step_key"]
        node["Sub - node Names"] = '; '.join(step_sub_node_mapping[step_key])
        node["step_key"] = step_key
        del node["_step_key"]

    gen_viz_reasoning = ' '.join([node["Reasoning"] for node in step_nodes])
    gen_viz_sub_names = '; '.join([node["Name"] for node in step_nodes])
    gen_viz_node = {
        "Name": "Generate Visualization",
        "Result": json_data.get("Step 5", {}).get("Final VQL", ""),
        "Reasoning": gen_viz_reasoning,
        "Sub - node Names": gen_viz_sub_names,
        "Status": 1,
        "Index": None,
        "Sub - node Indices": []
    }

    final_nodes = [gen_viz_node] + step_nodes + field_nodes

    for i, node in enumerate(final_nodes):
        node["Index"] = i

    step_keys_order = list(json_data.keys())
    step_key_to_final_index = {}
    for idx, step_key in enumerate(step_keys_order, start=1):
        if step_key == "Step 5":
            continue
        step_key_to_final_index[step_key] = idx

    for node in field_nodes:
        parent_key = node["parent"]
        parent_index = step_key_to_final_index.get(parent_key)
        if parent_index is not None:
            final_nodes[parent_index]["Sub - node Indices"].append(node["Index"])

    gen_viz_node["Sub - node Indices"] = list(step_key_to_final_index.values())

    for node in field_nodes:
        if "parent" in node:
            del node["parent"]
    for node in step_nodes:
        if "step_key" in node:
            del node["step_key"]

    logging.debug("Node structure conversion completed")
    return final_nodes


def run_generate(db_file, question_file, saved_model_folder, saved_adapter_folder):
    logging.info("[Generate] Begining")
    schema = generate_database_schema(db_file)
    if schema is None:
        logging.error("[Generate] Failed to generate database schema.")
        return
    db_schema_lines = format_schema(schema)

    logging.debug("[Generate] Read user question file：%s", question_file)
    try:
        with open(question_file, 'r', encoding='utf-8') as f:
            question_data = json.load(f)
            question = question_data.get("question", "")
        logging.debug("[Generate] User query read successfully：%s", question)
    except Exception as e:
        logging.error("[Generate] Error reading the question file: %s", e)
        return

    prompt_text = build_prompt(question, db_schema_lines)
    prompt_json_obj = [{
        "db_id": "",
        "role_1": "user",
        "content_1": prompt_text,
        "role_2": "assistant",
        "content_2": ""
    }]
    with open(GENERATED_PROMPT_FILE, 'w', encoding='utf-8') as f:
        json.dump(prompt_json_obj, f, ensure_ascii=False, indent=4)
    logging.info("[Generate] Initial prompt JSON has been saved to %s", GENERATED_PROMPT_FILE)

    json_data = load_json_data(GENERATED_PROMPT_FILE)
    #model, tokenizer = load_model(with_lora=True, saved_model_folder=saved_model_folder, saved_adapter_folder=saved_adapter_folder)
    results = []
    for item in tqdm(json_data, desc="[Generate] Processing prompt"):
        content_1 = extract_content_1(item)
        prompt_input = generate_input(content_1)
        response_text = generate_responses(model, tokenizer, prompt_input)
        result = {
            "prompt": prompt_input,
            "response_finetuned_model": response_text
        }
        results.append(result)
    with open(MODEL_RESPONSE_FILE, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    logging.info("[Generate] Response has been saved to %s", MODEL_RESPONSE_FILE)

    output_txt_path = process_json_file(MODEL_RESPONSE_FILE)
    if output_txt_path:
        try:
            with open(output_txt_path, 'r', encoding='utf-8') as f:
                txt_data = f.read()
            json_data_from_txt = convert_txt_to_json(txt_data)
            nodes = convert_json_to_nodes(json_data_from_txt)
            with open(NODE_JSON_FILE, 'w', encoding='utf-8') as f:
                json.dump(nodes, f, ensure_ascii=False, indent=4)
            logging.info("[Generate] Structured node has been saved to %s", NODE_JSON_FILE)
        except Exception as e:
            logging.error("[Generate] Error occurred during the structured node conversion process: %s", e)


def run_regenerate(corrections_file):
    logging.info("[Regenerate] Begining")
    try:
        with open(corrections_file, 'r', encoding='utf-8') as file:
            corrections_data = json.load(file)
    except Exception as e:
        logging.error("[Regenerate] Error reading corrections file: %s", e)
        return

    correction_prompts = []
    for correction in corrections_data.get('corrections', []):
        node_name = correction.get('node_name')
        correction_type = correction.get('type')
        value = correction.get('value')
        if correction_type == "self":
            correction_prompt = f'pay attention to the previous answer "{node_name}"'
        elif correction_type == "manual":
            correction_prompt = f'According to the suggestion "{value}" revise the previous answer "{node_name}".'
        else:
            correction_prompt = ""
        if correction_prompt:
            correction_prompts.append(correction_prompt)
    user_feedback = "\n".join(correction_prompts)
    logging.debug("[Regenerate] The user feedback has been generated：%s", user_feedback)

    try:
        with open(GENERATED_PROMPT_FILE, 'r', encoding='utf-8') as f:
            prompt_data = json.load(f)
        if prompt_data and isinstance(prompt_data, list):
            orig_prompt_obj = prompt_data[0]
            orig_prompt_text = orig_prompt_obj.get("content_1", "")
            question_section = orig_prompt_text.split("Question:")[1].split("Database Schema:")[0].strip()
        else:
            logging.error("[Regenerate] generated_prompt.json content format is abnormal.")
            return
    except Exception as e:
        logging.error(f"[Regenerate] read {GENERATED_PROMPT_FILE} error: {e}")
        return

    try:
        db_schema_section = orig_prompt_text.split("Database Schema:")[1].split("The VQL template")[0].strip()
        db_schema_lines = db_schema_section.splitlines()
    except Exception as e:
        logging.error("[Regenerate] There was an error parsing the database Schema: %s", e)
        return

    try:
        with open(OUTPUT_TXT_FILE, 'r', encoding='utf-8') as f:
            original_generated_response = f.read().strip()
    except Exception as e:
        logging.error("[Regenerate] Read %s error: %s", OUTPUT_TXT_FILE, e)
        original_generated_response = ""

    new_prompt_text = build_prompt_regenerate(question_section, db_schema_lines, user_feedback, original_generated_response)
    new_prompt_obj = [{
        "db_id": "",
        "role_1": "user",
        "content_1": new_prompt_text,
        "role_2": "assistant",
        "content_2": ""
    }]
    with open(GENERATED_PROMPT_NEW_FILE, 'w', encoding='utf-8') as f:
        json.dump(new_prompt_obj, f, ensure_ascii=False, indent=4)
    logging.info("[Regenerate] The regenerated prompt JSON has been saved to %s", GENERATED_PROMPT_NEW_FILE)

    json_data = load_json_data(GENERATED_PROMPT_NEW_FILE)
    #model, tokenizer = load_model(with_lora=True, saved_model_folder=SAVED_MODEL_FOLDER, saved_adapter_folder=SAVED_ADAPTER_FOLDER)
    results = []
    for item in tqdm(json_data, desc="[Regenerate] Processing new prompt"):
        content_1 = extract_content_1(item)
        prompt_input = generate_input(content_1)
        response_text = generate_responses(model, tokenizer, prompt_input)
        result = {
            "prompt": prompt_input,
            "response_finetuned_model": response_text
        }
        results.append(result)
    with open(MODEL_RESPONSE_NEW_FILE, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    logging.info("[Regenerate] The regenerated response from the large language model has been saved to %s", MODEL_RESPONSE_NEW_FILE)

    try:
        new_output_txt = process_json_file(MODEL_RESPONSE_NEW_FILE)
        if new_output_txt:
            with open(new_output_txt, 'r', encoding='utf-8') as f:
                txt_data = f.read()
            json_data_from_txt = convert_txt_to_json(txt_data)
            new_nodes = convert_json_to_nodes(json_data_from_txt)
            with open(NODE_JSON_NEW_FILE, 'w', encoding='utf-8') as f:
                json.dump(new_nodes, f, ensure_ascii=False, indent=4)
            logging.info("[Regenerate] The regenerated structured node information has been saved to %s", NODE_JSON_NEW_FILE)
        else:
            logging.error("[Regenerate] When processing the new response, a new output text file was not generated. ")
    except Exception as e:
        logging.error("[Regenerate] An error occurred during the process of regenerating the structured node conversion: %s", e)

# -------------------------------------------
# Flask
# -------------------------------------------
model, tokenizer = load_model(with_lora=True, saved_model_folder=SAVED_MODEL_FOLDER, saved_adapter_folder=SAVED_ADAPTER_FOLDER)
@app.route("/api/visualization/generate", methods=["POST"])
def generate_visualization():
    logging.info("Get generate Request")
    form = request.form
    files = request.files
    question = form.get("question")
    sqlite_file = files.get("sqlite_file")

    if not question or not sqlite_file:
        error_msg = ""
        if not question:
            error_msg += "Question is missing"
        if not sqlite_file:
            error_msg += "SQLite file is missing"
        return jsonify({"error": error_msg}), 400

    question_path = QUESTION_FILE
    try:
        with open(question_path, "w", encoding="utf-8") as f:
            json.dump({"question": question}, f, ensure_ascii=False, indent=2)
        logging.info("Saved question to %s", question_path)
    except Exception as e:
        logging.error("Error saving question: %s", e)

    sqlite_path = DB_FILE
    try:
        sqlite_file.save(sqlite_path)
        logging.info("Saved sqlite file to %s", sqlite_path)
    except Exception as e:
        logging.error("Error saving sqlite file: %s", e)

    if DYNAMIC_MODE:
        run_generate(DB_FILE, QUESTION_FILE, SAVED_MODEL_FOLDER, SAVED_ADAPTER_FOLDER)
        try:
            with open(NODE_JSON_FILE, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
        except Exception as e:
            logging.error("Failed to read node data: %s", e)
            nodes_data = []
    else:
        try:
            with open(NODES_FILE, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
        except Exception as e:
            logging.error("Failed to read static node file: %s", e)
            nodes_data = []

    SVG_FILE_1=vql_to_svg(DB_FILE, NODE_JSON_FILE)
    with open('vis/output.svg', 'w') as f:  
        f.write(SVG_FILE_1)  
    try:
        with open("vis/output.svg", "r", encoding="utf-8") as f:
            svg = f.read()
    except Exception as e:
        logging.error("Error reading svg file: %s", e)
        svg = ""

    return jsonify({
        "nodesData": nodes_data,
        "chartSVG": SVG_FILE_1
    })

@app.route("/api/visualization/regenerate", methods=["POST"])
def regenerate_visualization():
    logging.info("Get Regenerate Request")
    data = request.get_json()

    corrections = data.get("corrections") if data else None
    if not corrections:
        return jsonify({"error": "Unmarked nodes need correction."}), 400

    try:
        with open(CORRECTIONS_FILE, 'w', encoding='utf-8') as f:
            json.dump({"corrections": corrections}, f, ensure_ascii=False, indent=2)
        logging.info("Saved corrections to %s", CORRECTIONS_FILE)
    except Exception as e:
        logging.error("Error saving corrections: %s", e)

    if DYNAMIC_MODE:
        run_regenerate(CORRECTIONS_FILE)
        try:
            with open(NODE_JSON_NEW_FILE, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
        except Exception as e:
            logging.error("Failed to read regeneration node data: %s", e)
            nodes_data = []
    else:
        try:
            with open(NODES_FILE, 'r', encoding='utf-8') as f:
                nodes_data = json.load(f)
        except Exception as e:
            logging.error("Failed to read static node file: %s", e)
            nodes_data = []

    SVG_FILE_2=vql_to_svg(DB_FILE, NODE_JSON_FILE)
    try:
        with open(SVG_FILE_2, "r", encoding="utf-8") as f:
            svg = f.read()
    except Exception as e:
        logging.error("Error reading svg file: %s", e)
        svg = ""

    return jsonify({
        "nodesData": nodes_data,
        "chartSVG": SVG_FILE_2
    })

# -------------------------------------------
# main
# -------------------------------------------
if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    app.run(debug=True, port=5001)
